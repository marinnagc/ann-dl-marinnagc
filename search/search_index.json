{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Marinna Grigolli Cesar</li> <li>Grupo<ul> <li>Marinna Grigolli Cesar</li> <li>Nicholas</li> <li>Guilherme</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 05/09/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"roteiro1/exercises/","title":"Notebook 1 - Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixar semente para reprodutibilidade\nnp.random.seed(42)\n\n# N\u00famero de pontos por classe\nn = 100\n\n# Defini\u00e7\u00f5es das classes\nparams = {\n    0: {\"mean\": [2, 3], \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6], \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1], \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]}\n}\n</pre> import numpy as np import matplotlib.pyplot as plt  # Fixar semente para reprodutibilidade np.random.seed(42)  # N\u00famero de pontos por classe n = 100  # Defini\u00e7\u00f5es das classes params = {     0: {\"mean\": [2, 3], \"std\": [0.8, 2.5]},     1: {\"mean\": [5, 6], \"std\": [1.2, 1.9]},     2: {\"mean\": [8, 1], \"std\": [0.9, 0.9]},     3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]} } In\u00a0[2]: Copied! <pre># Gerar dados\ndata = []\nlabels = []\nfor label, p in params.items():\n    x = np.random.normal(p[\"mean\"][0], p[\"std\"][0], n) # np.random.normal gera n\u00fameros aleat\u00f3rios com distribui\u00e7\u00e3o normal\n    y = np.random.normal(p[\"mean\"][1], p[\"std\"][1], n)\n    points = np.column_stack((x, y)) # Combina x e y em uma matriz n x 2, np.column_stack junta arrays como colunas\n    data.append(points)\n    labels.append(np.full(n, label)) # np.full cria um array preenchido com o valor do label\n\n# Concatenar tudo\nX = np.vstack(data) # np.vstack empilha arrays verticalmente\ny = np.hstack(labels) # np.hstack empilha arrays horizontalmente\n</pre> # Gerar dados data = [] labels = [] for label, p in params.items():     x = np.random.normal(p[\"mean\"][0], p[\"std\"][0], n) # np.random.normal gera n\u00fameros aleat\u00f3rios com distribui\u00e7\u00e3o normal     y = np.random.normal(p[\"mean\"][1], p[\"std\"][1], n)     points = np.column_stack((x, y)) # Combina x e y em uma matriz n x 2, np.column_stack junta arrays como colunas     data.append(points)     labels.append(np.full(n, label)) # np.full cria um array preenchido com o valor do label  # Concatenar tudo X = np.vstack(data) # np.vstack empilha arrays verticalmente y = np.hstack(labels) # np.hstack empilha arrays horizontalmente In\u00a0[3]: Copied! <pre># ---------- Plot ----------\nplt.figure(figsize=(9,7))\ncolors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n\n# Scatter de cada classe\nfor i, c in enumerate(colors):\n    plt.scatter(X[y==i, 0], X[y==i, 1], alpha=0.7, label=f'Classe {i}', color=c)\n\n# Criar valores no eixo X para desenhar as retas\nx_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\n\n# ---------- Retas manuais (ajuste \u201cno olho\u201d) ----------\nplt.plot([3.1,3.1], [-2,12], 'k--', linewidth=1)   # entre Classe 0 e 1\nplt.plot([3.1,12.5], [1,5], 'k--', linewidth=1)  # entre Classe 1 e 2\nplt.plot([12.5,12.5], [-2,12], 'k--', linewidth=1)   # entre Classe 2 e 3\n\n# Ajustes do gr\u00e1fico\nplt.title(\"Distribui\u00e7\u00e3o das Classes e Fronteiras Lineares (esbo\u00e7adas)\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # ---------- Plot ---------- plt.figure(figsize=(9,7)) colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']  # Scatter de cada classe for i, c in enumerate(colors):     plt.scatter(X[y==i, 0], X[y==i, 1], alpha=0.7, label=f'Classe {i}', color=c)  # Criar valores no eixo X para desenhar as retas x_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)  # ---------- Retas manuais (ajuste \u201cno olho\u201d) ---------- plt.plot([3.1,3.1], [-2,12], 'k--', linewidth=1)   # entre Classe 0 e 1 plt.plot([3.1,12.5], [1,5], 'k--', linewidth=1)  # entre Classe 1 e 2 plt.plot([12.5,12.5], [-2,12], 'k--', linewidth=1)   # entre Classe 2 e 3  # Ajustes do gr\u00e1fico plt.title(\"Distribui\u00e7\u00e3o das Classes e Fronteiras Lineares (esbo\u00e7adas)\") plt.xlabel(\"X1\") plt.ylabel(\"X2\") plt.legend() plt.grid(True) plt.show() In\u00a0[14]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# M\u00e9dias\nmu_A = np.array([0, 0, 0, 0, 0])\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n\n# Matrizes de covari\u00e2ncia\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42)  # M\u00e9dias mu_A = np.array([0, 0, 0, 0, 0]) mu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])  # Matrizes de covari\u00e2ncia Sigma_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  Sigma_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])   In\u00a0[15]: Copied! <pre># Amostras\nn = 500\nXA = np.random.multivariate_normal(mu_A, Sigma_A, size=n)\nXB = np.random.multivariate_normal(mu_B, Sigma_B, size=n)\n\n# Dataset completo\nX = np.vstack([XA, XB])              # (1000, 5)\ny = np.hstack([np.zeros(n), np.ones(n)])  # r\u00f3tulos\n</pre> # Amostras n = 500 XA = np.random.multivariate_normal(mu_A, Sigma_A, size=n) XB = np.random.multivariate_normal(mu_B, Sigma_B, size=n)  # Dataset completo X = np.vstack([XA, XB])              # (1000, 5) y = np.hstack([np.zeros(n), np.ones(n)])  # r\u00f3tulos   <p>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions. Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</p> In\u00a0[16]: Copied! <pre># ===== PCA do zero =====\n# Centralizar\nXc = X - X.mean(axis=0)\n\n# Covari\u00e2ncia\nCov = np.cov(Xc, rowvar=False)\n\n# Autovalores e autovetores\nvals, vecs = np.linalg.eigh(Cov)\n\n# Ordenar\norder = np.argsort(vals)[::-1]\nvecs = vecs[:, order]\nvals = vals[order]\n\n# Projetar nos 2 primeiros PCs\nW = vecs[:, :2]\nZ = Xc @ W\n# ======================\n</pre> # ===== PCA do zero ===== # Centralizar Xc = X - X.mean(axis=0)  # Covari\u00e2ncia Cov = np.cov(Xc, rowvar=False)  # Autovalores e autovetores vals, vecs = np.linalg.eigh(Cov)  # Ordenar order = np.argsort(vals)[::-1] vecs = vecs[:, order] vals = vals[order]  # Projetar nos 2 primeiros PCs W = vecs[:, :2] Z = Xc @ W # ======================  In\u00a0[17]: Copied! <pre># Plotar\nplt.figure(figsize=(7,6))\nplt.scatter(Z[y==0,0], Z[y==0,1], s=14, alpha=0.7, label=\"Class A\")\nplt.scatter(Z[y==1,0], Z[y==1,1], s=14, alpha=0.7, label=\"Class B\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\n\nvar_exp = 100 * vals[:2].sum() / vals.sum()\nplt.title(f\"PCA 2D (vari\u00e2ncia explicada: {var_exp:.1f}%)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Plotar plt.figure(figsize=(7,6)) plt.scatter(Z[y==0,0], Z[y==0,1], s=14, alpha=0.7, label=\"Class A\") plt.scatter(Z[y==1,0], Z[y==1,1], s=14, alpha=0.7, label=\"Class B\") plt.xlabel(\"PC1\") plt.ylabel(\"PC2\")  var_exp = 100 * vals[:2].sum() / vals.sum() plt.title(f\"PCA 2D (vari\u00e2ncia explicada: {var_exp:.1f}%)\") plt.legend() plt.grid(True) plt.show()  <p>Dataset Description (Spaceship Titanic) Objetivo</p> <p>O dataset tem como objetivo prever se um passageiro foi transportado para outra dimens\u00e3o durante a colis\u00e3o da Spaceship Titanic com a anomalia do espa\u00e7o-tempo.</p> <p>A coluna Transported \u00e9 a vari\u00e1vel-alvo (target), representando:</p> <p>True \u2192 passageiro foi transportado.</p> <p>False \u2192 passageiro n\u00e3o foi transportado.</p> <p>Vari\u00e1veis (features)</p> <pre><code>Identifica\u00e7\u00e3o</code></pre> <p>PassengerId: Identificador \u00fanico de cada passageiro. N\u00e3o \u00e9 uma feature preditiva em si, mas pode ser usado para agrupar fam\u00edlias.</p> <pre><code>Categ\u00f3ricas (valores discretos / n\u00e3o num\u00e9ricos)</code></pre> <p>HomePlanet: Planeta de origem.</p> <p>CryoSleep: Indica se o passageiro estava em sono criog\u00eanico (True/False).</p> <p>Cabin: N\u00famero da cabine (pode ser decomposta em deck, num e side).</p> <p>Destination: Planeta de destino.</p> <p>VIP: Passageiro contratou servi\u00e7o VIP (True/False).</p> <p>Name: Nome do passageiro (pouco \u00fatil como feature, pode ser descartado ou usado apenas para engenharia de features).</p> <pre><code>Num\u00e9ricas (valores cont\u00ednuos / quantitativos)</code></pre> <p>Age: Idade.</p> <p>RoomService: Valor gasto em servi\u00e7o de quarto.</p> <p>FoodCourt: Valor gasto no restaurante.</p> <p>ShoppingMall: Valor gasto no shopping.</p> <p>Spa: Valor gasto no spa.</p> <p>VRDeck: Valor gasto no deck de realidade virtual.</p> <p>Target (vari\u00e1vel resposta)</p> <p>Transported: Passageiro foi transportado (True ou False).</p> In\u00a0[18]: Copied! <pre>import pandas as pd\n\n# Carregar dataset\ndf = pd.read_csv(\"spaceship-titanic/train.csv\")\n\n# Contar valores ausentes por coluna\nmissing = df.isnull().sum()\nprint(missing)\n</pre> import pandas as pd  # Carregar dataset df = pd.read_csv(\"spaceship-titanic/train.csv\")  # Contar valores ausentes por coluna missing = df.isnull().sum() print(missing) <pre>PassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</pre> <p>Valores ausentes</p> <p>No Kaggle, sabemos que o dataset tem missing values em v\u00e1rias colunas. Por exemplo:</p> <p>Age \u2192 alguns passageiros n\u00e3o t\u00eam idade registrada.</p> <p>Cabin \u2192 muitos registros est\u00e3o vazios.</p> <p>HomePlanet \u2192 h\u00e1 valores ausentes.</p> <p>CryoSleep e VIP \u2192 tamb\u00e9m t\u00eam valores faltantes.</p> <p>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2192 v\u00e1rios passageiros t\u00eam valores nulos (pode ser gasto zero ou dado faltante).</p> In\u00a0[19]: Copied! <pre>import warnings\n\n# Fill numeric with median\nfor col in [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]:\n    df[col].fillna(df[col].median(), inplace=True)\n\n# Fill categorical with mode\nfor col in [\"HomePlanet\",\"Destination\"]:\n    df[col].fillna(df[col].mode()[0], inplace=True)\n\n# Binary categorical\nfor col in [\"CryoSleep\",\"VIP\"]:\n    df[col].fillna(df[col].mode()[0], inplace=True)\n\n# Cabin -&gt; split\ndf[\"Deck\"] = df[\"Cabin\"].str[0]\ndf[\"Side\"] = df[\"Cabin\"].str[-1]\ndf.drop(columns=[\"Cabin\",\"Name\",\"PassengerId\"], inplace=True)\ndf[\"Deck\"].fillna(\"Unknown\", inplace=True)\ndf[\"Side\"].fillna(\"Unknown\", inplace=True)\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  # Fill numeric with median for col in [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]:     df[col].fillna(df[col].median(), inplace=True)  # Fill categorical with mode for col in [\"HomePlanet\",\"Destination\"]:     df[col].fillna(df[col].mode()[0], inplace=True)  # Binary categorical for col in [\"CryoSleep\",\"VIP\"]:     df[col].fillna(df[col].mode()[0], inplace=True)  # Cabin -&gt; split df[\"Deck\"] = df[\"Cabin\"].str[0] df[\"Side\"] = df[\"Cabin\"].str[-1] df.drop(columns=[\"Cabin\",\"Name\",\"PassengerId\"], inplace=True) df[\"Deck\"].fillna(\"Unknown\", inplace=True) df[\"Side\"].fillna(\"Unknown\", inplace=True) warnings.filterwarnings(\"ignore\") In\u00a0[10]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnum_cols = [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n</pre> from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() num_cols = [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"] df[num_cols] = scaler.fit_transform(df[num_cols])  In\u00a0[11]: Copied! <pre>import numpy as np\n\nnum_cols = [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\n\nfor col in num_cols:\n    mean = df[col].mean()\n    std = df[col].std()\n    df[col] = (df[col] - mean) / std\n</pre> import numpy as np  num_cols = [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]  for col in num_cols:     mean = df[col].mean()     std = df[col].std()     df[col] = (df[col] - mean) / std In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(10,6))\n\n# Age antes\ndf_raw = pd.read_csv(\"spaceship-titanic/train.csv\")\naxes[0,0].hist(df_raw[\"Age\"].dropna(), bins=30, color=\"skyblue\")\naxes[0,0].set_title(\"Age - Before Scaling\")\n\n# Age depois\naxes[0,1].hist(df[\"Age\"], bins=30, color=\"green\")\naxes[0,1].set_title(\"Age - After Scaling\")\n\n# FoodCourt antes\naxes[1,0].hist(df_raw[\"FoodCourt\"].dropna(), bins=30, color=\"skyblue\")\naxes[1,0].set_title(\"FoodCourt - Before Scaling\")\n\n# FoodCourt depois\naxes[1,1].hist(df[\"FoodCourt\"], bins=30, color=\"green\")\naxes[1,1].set_title(\"FoodCourt - After Scaling\")\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, axes = plt.subplots(2, 2, figsize=(10,6))  # Age antes df_raw = pd.read_csv(\"spaceship-titanic/train.csv\") axes[0,0].hist(df_raw[\"Age\"].dropna(), bins=30, color=\"skyblue\") axes[0,0].set_title(\"Age - Before Scaling\")  # Age depois axes[0,1].hist(df[\"Age\"], bins=30, color=\"green\") axes[0,1].set_title(\"Age - After Scaling\")  # FoodCourt antes axes[1,0].hist(df_raw[\"FoodCourt\"].dropna(), bins=30, color=\"skyblue\") axes[1,0].set_title(\"FoodCourt - Before Scaling\")  # FoodCourt depois axes[1,1].hist(df[\"FoodCourt\"], bins=30, color=\"green\") axes[1,1].set_title(\"FoodCourt - After Scaling\")  plt.tight_layout() plt.show()"},{"location":"roteiro1/exercises/#notebook-1-data","title":"Notebook 1 - Data\u00b6","text":"<p>Activity: Data Preparation and Analysis for Neural Networks</p> <p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"roteiro1/exercises/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Exploring Class Separability in 2D</p> <p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"roteiro1/exercises/#11-generate-the-data","title":"1.1 Generate the Data:\u00b6","text":"<p>Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:</p>"},{"location":"roteiro1/exercises/#12-plot-the-data","title":"1.2 Plot the Data:\u00b6","text":"<p>Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p>"},{"location":"roteiro1/exercises/#13-analyze-and-draw-boundaries","title":"1.3 Analyze and Draw Boundaries:\u00b6","text":"<p>a. Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</p> <p>b. Based on your visual inspection, could a simple, linear boundary separate all classes?</p> <p>c. On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</p>"},{"location":"roteiro1/exercises/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Non-Linearity in Higher Dimensions</p> <p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"roteiro1/exercises/#21-generate-the-data","title":"2.1 Generate the Data:\u00b6","text":"<p>Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p>"},{"location":"roteiro1/exercises/#visualize-the-data","title":"Visualize the Data:\u00b6","text":"<p>Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p>"},{"location":"roteiro1/exercises/#analyze-the-plots","title":"Analyze the Plots:\u00b6","text":"<p>a. Based on your 2D projection, describe the relationship between the two classes.</p> <p>b. Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</p>"},{"location":"roteiro1/exercises/#exercise-3","title":"Exercise 3\u00b6","text":""},{"location":"roteiro1/exercises/#preparing-real-world-data-for-a-neural-network","title":"Preparing Real-World Data for a Neural Network\u00b6","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"roteiro1/exercises/#dataset-spaceship-titanic","title":"DATASET: Spaceship Titanic\u00b6","text":""},{"location":"roteiro1/exercises/#31-preprocessing","title":"3.1 Preprocessing\u00b6","text":"<p>a) Handling Missing Data</p> <p>Num\u00e9ricas (Age, gastos) \u2192 substituir pela mediana (mais robusta a outliers).</p> <p>Categ\u00f3ricas (HomePlanet, Destination) \u2192 substituir pelo valor mais frequente (mode).</p> <p>Bin\u00e1rias (CryoSleep, VIP) \u2192 preencher com False ou pelo mode.</p> <p>Cabin \u2192 separar em Deck e Side; valores ausentes tratados como categoria \"Unknown\".</p>"},{"location":"roteiro1/exercises/#32-normalize-standardize-numerical-features","title":"3.2 Normalize / Standardize Numerical Features\u00b6","text":"<p>Como vamos usar tanh, \u00e9 melhor que os dados estejam centrados em 0 e dentro de uma escala compar\u00e1vel.</p> <p>Op\u00e7\u00e3o 1: Padroniza\u00e7\u00e3o (Z-score) \u2192 m\u00e9dia = 0, desvio = 1.</p> <p>Op\u00e7\u00e3o 2: Normaliza\u00e7\u00e3o [-1, 1].</p> <p>Aqui vou usar Z-score:</p>"},{"location":"roteiro1/exercises/#33-visualizacao-antes-e-depois","title":"3.3 Visualiza\u00e7\u00e3o (antes e depois)\u00b6","text":"<p>Exemplo para Age e FoodCourt:</p>"},{"location":"roteiro1/main/","title":"1. Data","text":"<p>Atividade: Prepara\u00e7\u00e3o e An\u00e1lise de Dados para Redes Neurais</p> <p>Esta atividade foi projetada para testar suas habilidades na gera\u00e7\u00e3o de conjuntos de dados sint\u00e9ticos, no tratamento de desafios de dados do mundo real e na prepara\u00e7\u00e3o de dados para serem inseridos em redes neurais.</p>"},{"location":"roteiro1/main/#11-analisar-e-tracar-limites","title":"1.1 Analisar e Tra\u00e7ar Limites","text":"<p>(Exerc\u00edcio 1)</p> <p>Neste exerc\u00edcio exploramos a separabilidade em 2D para entender como diferentes distribui\u00e7\u00f5es de classes afetam a defini\u00e7\u00e3o de fronteiras de decis\u00e3o.</p> <p></p> <p>Distribui\u00e7\u00e3o das classes:</p> <ul> <li>A Classe 0 (azul) est\u00e1 concentrada em torno do ponto <code>[2, 3]</code>, com maior dispers\u00e3o no eixo vertical (X2).</li> <li>A Classe 1 (laranja) aparece pr\u00f3xima a <code>[5, 6]</code>, relativamente mais acima, mas parcialmente sobreposta \u00e0 Classe 0 no eixo X1.</li> <li>A Classe 2 (verde) est\u00e1 em <code>[8, 1]</code>, mais isolada, formando um cluster compacto e bem separado das Classes 0 e 1.</li> <li>A Classe 3 (vermelha) est\u00e1 distante, em <code>[15, 4]</code>, praticamente sem sobreposi\u00e7\u00e3o com as demais.</li> </ul> <p>Sobreposi\u00e7\u00e3o:</p> <ul> <li>Existe sobreposi\u00e7\u00e3o vis\u00edvel entre Classe 0 e Classe 1, principalmente nos limites superiores e inferiores.</li> <li>As Classes 2 e 3 aparecem mais isoladas e s\u00e3o mais facilmente separ\u00e1veis.</li> </ul> <p>Limites lineares:</p> <ul> <li>Uma \u00fanica fronteira linear n\u00e3o conseguiria separar todas as classes simultaneamente.</li> <li> <p>Algumas classes, no entanto, poderiam ser separadas com limites simples:</p> </li> <li> <p>Classe 3 pode ser isolada com uma linha vertical.</p> </li> <li>Classe 2 pode ser separada da Classe 0/1 com uma linha inclinada.</li> <li>Classe 0 e 1 exigem uma fronteira mais complexa devido \u00e0 sobreposi\u00e7\u00e3o.</li> </ul> <p>Rede Neural:</p> <ul> <li>Uma rede neural com fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares (ex.: <code>tanh</code> ou <code>ReLU</code>) poderia aprender fronteiras curvas para separar Classe 0 e Classe 1 com maior precis\u00e3o.</li> <li>Assim, enquanto limites lineares funcionam para parte do espa\u00e7o, apenas uma rede neural \u00e9 capaz de capturar fronteiras mais flex\u00edveis e reduzir erros nas regi\u00f5es de sobreposi\u00e7\u00e3o.</li> </ul>"},{"location":"roteiro1/main/#12-nao-linearidade-em-dimensoes-superiores","title":"1.2 N\u00e3o-linearidade em dimens\u00f5es superiores","text":"<p>(Exerc\u00edcio 2)</p> <p>Este exerc\u00edcio demonstra a import\u00e2ncia da redu\u00e7\u00e3o de dimensionalidade e mostra como dados em dimens\u00f5es mais altas podem se tornar complexos para modelos lineares.</p> <p></p> <p>Relacionamento entre as classes:</p> <p>Na proje\u00e7\u00e3o 2D obtida via PCA, as Classes A (azul) e B (laranja) aparecem como dois grupos com regi\u00f5es predominantes distintas. A Classe A se concentra mais \u00e0 direita do eixo PC1, enquanto a Classe B aparece mais \u00e0 esquerda. Entretanto, h\u00e1 uma regi\u00e3o central significativa em que os pontos das duas classes se sobrep\u00f5em, indicando que n\u00e3o existe separa\u00e7\u00e3o perfeita na proje\u00e7\u00e3o.</p> <p>Separabilidade linear:</p> <p>Os dados n\u00e3o s\u00e3o totalmente separ\u00e1veis por uma fronteira linear simples. Uma reta at\u00e9 poderia dividir aproximadamente as classes em lados opostos, mas haveria muitos erros de classifica\u00e7\u00e3o devido \u00e0 sobreposi\u00e7\u00e3o. Isso mostra que modelos lineares n\u00e3o conseguem capturar toda a complexidade da distribui\u00e7\u00e3o.</p> <p>Por que \u00e9 um desafio para modelos lineares:</p> <p>Esse tipo de estrutura de dados \u00e9 desafiador porque as classes n\u00e3o seguem fronteiras lineares bem definidas. A diferen\u00e7a de covari\u00e2ncias faz com que os aglomerados tenham formatos e dispers\u00f5es distintas, e a proje\u00e7\u00e3o em 2D comprime ainda mais a separabilidade. Assim, um modelo linear simples n\u00e3o seria capaz de tra\u00e7ar fronteiras adequadas, enquanto uma rede neural multicamadas com fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares pode aprender fronteiras de decis\u00e3o curvas e adaptativas, reduzindo os erros nas \u00e1reas de sobreposi\u00e7\u00e3o.</p>"},{"location":"roteiro1/main/#13-preparando-dados-do-mundo-real-para-uma-rede-neural","title":"1.3 Preparando dados do mundo real para uma rede neural","text":"<p>(Exerc\u00edcio 3)</p> <p>Este exerc\u00edcio usa o dataset real Spaceship Titanic para demonstrar como dados brutos precisam ser limpos e transformados antes de alimentar uma rede neural com fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>.</p> <p></p> <p>Objetivo do dataset Spaceship Titanic:</p> <ul> <li>Prever se um passageiro foi transportado para outra dimens\u00e3o ap\u00f3s a colis\u00e3o da nave.</li> <li>A vari\u00e1vel alvo \u00e9 <code>Transported</code>, bin\u00e1ria (<code>True</code>/<code>False</code>).</li> </ul> <p>Features:</p> <ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code> (decomposta em <code>Deck</code> e <code>Side</code>), <code>Destination</code>, <code>VIP</code>, <code>Name</code>.</li> </ul> <p>Valores ausentes:</p> <ul> <li>Detectados em diversas colunas: <code>Age</code>, <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>, <code>Cabin</code> e nos gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>).</li> <li>Isso refor\u00e7a a necessidade de um pr\u00e9-processamento adequado antes do treino.</li> </ul> <p>Tratamento de missing values:</p> <ul> <li>Num\u00e9ricas: preenchidas com a mediana (robusta a outliers).</li> <li>Categ\u00f3ricas: substitu\u00eddas pelo valor mais frequente (mode).</li> <li>Booleanas: (<code>CryoSleep</code>, <code>VIP</code>) \u2192 preenchidas com <code>False</code>.</li> <li> <p>Cabin: decomposta em <code>Deck</code> e <code>Side</code>, ausentes preenchidos como <code>\"Unknown\"</code>.</p> </li> <li> <p>Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas:</p> </li> <li> <p>Aplicado One-Hot Encoding, criando colunas bin\u00e1rias para cada categoria.</p> </li> <li>Essa abordagem evita impor rela\u00e7\u00f5es artificiais entre categorias (ex.: <code>HomePlanet</code> \u2260 ordinal).</li> </ul> <p>Escalonamento (scaling):</p> <ul> <li>Aplicada padroniza\u00e7\u00e3o (Z-score) \u00e0s vari\u00e1veis num\u00e9ricas.</li> <li>F\u00f3rmula: \\(x' = \\frac{x - \\mu}{\\sigma}\\).</li> <li>Justificativa: como a fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh gera sa\u00eddas em [-1,1] e \u00e9 centrada em 0, dados padronizados aceleram o aprendizado e evitam satura\u00e7\u00e3o em valores extremos.</li> </ul> <p>Resultados dos gr\u00e1ficos (antes vs depois do scaling):</p> <ul> <li>Age: antes variava de 0 a 80, concentrada entre 20\u201340. Depois do scaling, a distribui\u00e7\u00e3o ficou centrada em 0, variando entre aproximadamente -2 e +3. Isso mostra que a padroniza\u00e7\u00e3o funcionou corretamente.</li> <li>FoodCourt: antes apresentava forte assimetria, com a maioria dos valores pr\u00f3ximos de 0 e alguns passageiros com gastos alt\u00edssimos (outliers acima de 20.000). Depois do scaling, a vari\u00e1vel tamb\u00e9m ficou ajustada em torno de 0, mas a assimetria permaneceu vis\u00edvel, evidenciando que o Z-score n\u00e3o elimina outliers, apenas ajusta escala e m\u00e9dia.</li> <li>Essa an\u00e1lise mostra que vari\u00e1veis como <code>Age</code> ficam mais bem distribu\u00eddas ap\u00f3s padroniza\u00e7\u00e3o, enquanto vari\u00e1veis de gastos poderiam se beneficiar de uma transforma\u00e7\u00e3o extra (ex.: log) antes do scaling.</li> </ul> <p>Conclus\u00e3o \u2013 impacto do pr\u00e9-processamento</p> <p>O pr\u00e9-processamento foi essencial para tornar o conjunto de dados adequado ao treinamento de uma rede neural com fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>. O tratamento de valores ausentes garantiu consist\u00eancia, enquanto a codifica\u00e7\u00e3o one-hot transformou vari\u00e1veis categ\u00f3ricas em formato num\u00e9rico sem distor\u00e7\u00f5es. A padroniza\u00e7\u00e3o centrou e normalizou as vari\u00e1veis num\u00e9ricas, alinhando-as \u00e0 faixa de sa\u00edda da <code>tanh</code> ([-1,1]) e facilitando a converg\u00eancia do modelo. A an\u00e1lise gr\u00e1fica mostrou ganhos claros, como no caso de <code>Age</code>, que ficou bem distribu\u00edda ap\u00f3s o scaling, e destacou limita\u00e7\u00f5es, como em <code>FoodCourt</code>, onde outliers mantiveram a assimetria. No geral, o processo resultou em dados limpos, consistentes e devidamente escalados, formando uma base s\u00f3lida para o treinamento de redes neurais.</p>"},{"location":"roteiro2/exercises/","title":"Exercises","text":"<p>Perceptron do zero (sem classe) + GIF da reta a cada \u00e9poca \u2013 Exerc\u00edcios 1 e 2</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib import animation <p>=============== utilidades de dados ===============</p> In\u00a0[\u00a0]: Copied! <pre>def set_seed(seed=42):\n    np.random.seed(seed)\n</pre> def set_seed(seed=42):     np.random.seed(seed) In\u00a0[\u00a0]: Copied! <pre>def make_dataset(mean0, cov0, mean1, cov1, n_per_class=1000, seed=42):\n    set_seed(seed)\n    X0 = np.random.multivariate_normal(mean0, cov0, n_per_class)\n    X1 = np.random.multivariate_normal(mean1, cov1, n_per_class)\n    X = np.vstack([X0, X1])\n    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)]).astype(int)\n    # embaralha\n    idx = np.arange(len(X))\n    np.random.shuffle(idx)\n    return X[idx], y[idx]\n</pre> def make_dataset(mean0, cov0, mean1, cov1, n_per_class=1000, seed=42):     set_seed(seed)     X0 = np.random.multivariate_normal(mean0, cov0, n_per_class)     X1 = np.random.multivariate_normal(mean1, cov1, n_per_class)     X = np.vstack([X0, X1])     y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)]).astype(int)     # embaralha     idx = np.arange(len(X))     np.random.shuffle(idx)     return X[idx], y[idx] <p>=============== perceptron ===============</p> In\u00a0[\u00a0]: Copied! <pre>def perceptron_fit(X, y, lr=0.01, max_epochs=100, seed=0):\n    \"\"\"\n    Treina perceptron online tradicional (sem classe).\n    Retorna um dicion\u00e1rio com hist\u00f3rico para animar/avaliar.\n    Perceptron online: atualiza w,b a cada amostra.\n    \"\"\"\n    set_seed(seed) # para reprodutibilidade\n    n, d = X.shape # n amostras, d dimens\u00f5es (features)\n    w = np.random.randn(d) * 0.01 # inicializa w pequeno\n    b = 0.0                     # inicializa b = 0 (bias)\n\n    ws, bs, accs, preds_hist = [], [], [], [] # hist\u00f3rico por \u00e9poca\n    converged = False # se parou antes do max_epochs\n    epochs_run = max_epochs # quantas \u00e9pocas rodou de fato \n\n    for epoch in range(1, max_epochs + 1): \n        updates = 0\n        for i in range(n):\n            xi = X[i] # amostra i\n            yi = y[i] # label i\n            y_hat = 1 if (np.dot(xi, w) + b) &gt;= 0 else 0 # predi\u00e7\u00e3o (y predito bin\u00e1rio)\n            if y_hat != yi: # s\u00f3 atualiza se errou\n                delta = (yi - y_hat)          # +1 ou -1\n                w = w + lr * delta * xi # atualiza w\n                b = b + lr * delta    # atualiza b\n                updates += 1\n\n        # fim da \u00e9poca: calcula acur\u00e1cia\n\n        scores = X @ w + b \n        '''\n        @ em Python faz multiplica\u00e7\u00e3o de matrizes (produto matricial)\n        X: geralmente \u00e9 uma matriz (por exemplo, de dados de entrada, onde cada linha \u00e9 \n                                    uma amostra e cada coluna \u00e9 uma caracter\u00edstica/feature).\n        w: normalmente \u00e9 um vetor de pesos (ou coeficientes) para cada caracter\u00edstica.\n        b: \u00e9 um vi\u00e9s (bias), geralmente um escalar ou vetor.\n\n        Calcula uma combina\u00e7\u00e3o linear dos dados de entrada (X), ponderada pelos pesos (w),\n        e soma o vi\u00e9s (b). O resultado (scores) normalmente representa as predi\u00e7\u00f5es de um \n        modelo linear, como regress\u00e3o linear ou a camada de sa\u00edda de uma rede neural simples.\n        '''\n        y_pred = (scores &gt;= 0).astype(int)  # essa linha transforma os scores lineares em predi\u00e7\u00f5es bin\u00e1rias (0 ou 1), \n                                            # usando a fun\u00e7\u00e3o degrau (step function) do perceptron.\n        '''\n        o score pode ser negativo, zero ou positivo dependendo de onde o ponto\n        x est\u00e1 em rela\u00e7\u00e3o \u00e0 reta (no 2D) ou hiperplano (em dimens\u00f5es maiores)\n        O zero \u00e9 exatamente a fronteira de decis\u00e3o: os pontos que satisfazem\n        w\u22c5x+b=0\n        est\u00e3o em cima da reta de decis\u00e3o.\n        Se um ponto d\u00e1 score &gt; 0, ele est\u00e1 de um lado da reta.\n        Se d\u00e1 score &lt; 0, ele est\u00e1 do outro lado.\n        '''\n        acc = (y_pred == y).mean() # acur\u00e1cia da \u00e9poca, m\u00e9dia de acertos\n                                   # (y_pred==y) \u00e9 um array booleano, compara elemento a elemento,\n                                   # True=acertou, False=errou; .mean() d\u00e1 a m\u00e9dia de acertos (True=1, False=0)\n        # salva hist\u00f3rico\n        ws.append(w.copy())\n        bs.append(float(b))\n        accs.append(float(acc))\n        preds_hist.append(y_pred)\n        # crit\u00e9rio de parada: se n\u00e3o teve atualiza\u00e7\u00e3o, parou\n        # como assim se nao teve atualizacao? se o perceptron acertou todas as amostras\n        if updates == 0:\n            converged = True # parou antes do max_epochs\n            epochs_run = epoch # quantas \u00e9pocas rodou de fato\n            break\n\n    return {\n        \"ws\": ws,             # lista de w por \u00e9poca (np.array shape (2,))\n        \"bs\": bs,             # lista de b por \u00e9poca (float)\n        \"accs\": accs,         # acur\u00e1cia por \u00e9poca\n        \"preds_hist\": preds_hist,  # predi\u00e7\u00f5es por \u00e9poca\n        \"converged\": converged,\n        \"epochs_run\": epochs_run,\n        \"w_final\": ws[-1],\n        \"b_final\": bs[-1],\n        \"y_pred_final\": preds_hist[-1],\n    }\n</pre> def perceptron_fit(X, y, lr=0.01, max_epochs=100, seed=0):     \"\"\"     Treina perceptron online tradicional (sem classe).     Retorna um dicion\u00e1rio com hist\u00f3rico para animar/avaliar.     Perceptron online: atualiza w,b a cada amostra.     \"\"\"     set_seed(seed) # para reprodutibilidade     n, d = X.shape # n amostras, d dimens\u00f5es (features)     w = np.random.randn(d) * 0.01 # inicializa w pequeno     b = 0.0                     # inicializa b = 0 (bias)      ws, bs, accs, preds_hist = [], [], [], [] # hist\u00f3rico por \u00e9poca     converged = False # se parou antes do max_epochs     epochs_run = max_epochs # quantas \u00e9pocas rodou de fato       for epoch in range(1, max_epochs + 1):          updates = 0         for i in range(n):             xi = X[i] # amostra i             yi = y[i] # label i             y_hat = 1 if (np.dot(xi, w) + b) &gt;= 0 else 0 # predi\u00e7\u00e3o (y predito bin\u00e1rio)             if y_hat != yi: # s\u00f3 atualiza se errou                 delta = (yi - y_hat)          # +1 ou -1                 w = w + lr * delta * xi # atualiza w                 b = b + lr * delta    # atualiza b                 updates += 1          # fim da \u00e9poca: calcula acur\u00e1cia          scores = X @ w + b          '''         @ em Python faz multiplica\u00e7\u00e3o de matrizes (produto matricial)         X: geralmente \u00e9 uma matriz (por exemplo, de dados de entrada, onde cada linha \u00e9                                      uma amostra e cada coluna \u00e9 uma caracter\u00edstica/feature).         w: normalmente \u00e9 um vetor de pesos (ou coeficientes) para cada caracter\u00edstica.         b: \u00e9 um vi\u00e9s (bias), geralmente um escalar ou vetor.          Calcula uma combina\u00e7\u00e3o linear dos dados de entrada (X), ponderada pelos pesos (w),         e soma o vi\u00e9s (b). O resultado (scores) normalmente representa as predi\u00e7\u00f5es de um          modelo linear, como regress\u00e3o linear ou a camada de sa\u00edda de uma rede neural simples.         '''         y_pred = (scores &gt;= 0).astype(int)  # essa linha transforma os scores lineares em predi\u00e7\u00f5es bin\u00e1rias (0 ou 1),                                              # usando a fun\u00e7\u00e3o degrau (step function) do perceptron.         '''         o score pode ser negativo, zero ou positivo dependendo de onde o ponto         x est\u00e1 em rela\u00e7\u00e3o \u00e0 reta (no 2D) ou hiperplano (em dimens\u00f5es maiores)         O zero \u00e9 exatamente a fronteira de decis\u00e3o: os pontos que satisfazem         w\u22c5x+b=0         est\u00e3o em cima da reta de decis\u00e3o.         Se um ponto d\u00e1 score &gt; 0, ele est\u00e1 de um lado da reta.         Se d\u00e1 score &lt; 0, ele est\u00e1 do outro lado.         '''         acc = (y_pred == y).mean() # acur\u00e1cia da \u00e9poca, m\u00e9dia de acertos                                    # (y_pred==y) \u00e9 um array booleano, compara elemento a elemento,                                    # True=acertou, False=errou; .mean() d\u00e1 a m\u00e9dia de acertos (True=1, False=0)         # salva hist\u00f3rico         ws.append(w.copy())         bs.append(float(b))         accs.append(float(acc))         preds_hist.append(y_pred)         # crit\u00e9rio de parada: se n\u00e3o teve atualiza\u00e7\u00e3o, parou         # como assim se nao teve atualizacao? se o perceptron acertou todas as amostras         if updates == 0:             converged = True # parou antes do max_epochs             epochs_run = epoch # quantas \u00e9pocas rodou de fato             break      return {         \"ws\": ws,             # lista de w por \u00e9poca (np.array shape (2,))         \"bs\": bs,             # lista de b por \u00e9poca (float)         \"accs\": accs,         # acur\u00e1cia por \u00e9poca         \"preds_hist\": preds_hist,  # predi\u00e7\u00f5es por \u00e9poca         \"converged\": converged,         \"epochs_run\": epochs_run,         \"w_final\": ws[-1],         \"b_final\": bs[-1],         \"y_pred_final\": preds_hist[-1],     } <p>=============== plots e GIFs ===============</p> In\u00a0[\u00a0]: Copied! <pre>def compute_decision_line(w, b, x_min, x_max):\n    \"\"\"y = -(w1/w2)*x - b/w2 (ou linha vertical se w2 ~ 0).\"\"\"\n    if abs(w[1]) &gt; 1e-12:\n        xs = np.linspace(x_min, x_max, 200)\n        ys = -(w[0] / w[1]) * xs - b / w[1]\n        return xs, ys, None\n    elif abs(w[0]) &gt; 1e-12:\n        # vertical em x = -b/w1\n        return None, None, -b / w[0]\n    else:\n        return None, None, None\n</pre> def compute_decision_line(w, b, x_min, x_max):     \"\"\"y = -(w1/w2)*x - b/w2 (ou linha vertical se w2 ~ 0).\"\"\"     if abs(w[1]) &gt; 1e-12:         xs = np.linspace(x_min, x_max, 200)         ys = -(w[0] / w[1]) * xs - b / w[1]         return xs, ys, None     elif abs(w[0]) &gt; 1e-12:         # vertical em x = -b/w1         return None, None, -b / w[0]     else:         return None, None, None In\u00a0[\u00a0]: Copied! <pre>def plot_accuracy_curve(accs, title):\n    plt.figure(figsize=(6, 4))\n    plt.plot(np.arange(1, len(accs) + 1), accs, lw=2)\n    plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_accuracy_curve(accs, title):     plt.figure(figsize=(6, 4))     plt.plot(np.arange(1, len(accs) + 1), accs, lw=2)     plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\")     plt.title(title)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def make_gif(X, y, history, out_gif, title, fps=8):\n    \"\"\"\n    Cria GIF animando a reta de decis\u00e3o a cada \u00e9poca.\n    Usa matplotlib.animation com writer='pillow'.\n    \"\"\"\n    ws = history[\"ws\"]\n    bs = history[\"bs\"]\n    preds_hist = history[\"preds_hist\"]\n    epochs = len(ws)\n\n    # limites do scatter\n    pad = 1.0\n    x_min, x_max = X[:,0].min() - pad, X[:,0].max() + pad\n    y_min, y_max = X[:,1].min() - pad, X[:,1].max() + pad\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    def init():\n        ax.clear()\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")\n        ax.set_title(title)\n        return []\n\n    def update(frame):\n        ax.clear()\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")\n\n        # dados\n        ax.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0')\n        ax.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1')\n\n        # misclassificados nessa \u00e9poca\n        # o que \u00e9 misclassificados nessa \u00e9poca?\n        # s\u00e3o os pontos que o perceptron errou na predi\u00e7\u00e3o\n        y_pred = preds_hist[frame]\n        mis = (y_pred != y)\n        if mis.any():\n            ax.scatter(X[mis,0], X[mis,1], s=28, marker='x', label='Misclass.')\n\n        # reta de decis\u00e3o\n        w = ws[frame]; b = bs[frame]\n        xs, ys, x_vertical = compute_decision_line(w, b, x_min, x_max)\n        if xs is not None:\n            ax.plot(xs, ys, lw=2, label='Limite de decis\u00e3o')\n        elif x_vertical is not None:\n            ax.axvline(x_vertical, lw=2, label='Limite de decis\u00e3o')\n\n        # t\u00edtulo com \u00e9poca e acur\u00e1cia\n        acc = history[\"accs\"][frame]\n        ax.set_title(f\"{title} \u2014 \u00c9poca {frame+1}/{epochs} \u00b7 acc={acc:.3f}\")\n        ax.legend(loc=\"best\")\n        return []\n\n    anim = animation.FuncAnimation(\n        fig, update, init_func=init, frames=epochs, interval=1000//fps, blit=False\n    )\n\n    # salva GIF\n    anim.save(out_gif, writer='pillow', fps=fps)\n    plt.close(fig)\n</pre> def make_gif(X, y, history, out_gif, title, fps=8):     \"\"\"     Cria GIF animando a reta de decis\u00e3o a cada \u00e9poca.     Usa matplotlib.animation com writer='pillow'.     \"\"\"     ws = history[\"ws\"]     bs = history[\"bs\"]     preds_hist = history[\"preds_hist\"]     epochs = len(ws)      # limites do scatter     pad = 1.0     x_min, x_max = X[:,0].min() - pad, X[:,0].max() + pad     y_min, y_max = X[:,1].min() - pad, X[:,1].max() + pad      fig, ax = plt.subplots(figsize=(6, 6))      def init():         ax.clear()         ax.set_xlim(x_min, x_max)         ax.set_ylim(y_min, y_max)         ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")         ax.set_title(title)         return []      def update(frame):         ax.clear()         ax.set_xlim(x_min, x_max)         ax.set_ylim(y_min, y_max)         ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")          # dados         ax.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0')         ax.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1')          # misclassificados nessa \u00e9poca         # o que \u00e9 misclassificados nessa \u00e9poca?         # s\u00e3o os pontos que o perceptron errou na predi\u00e7\u00e3o         y_pred = preds_hist[frame]         mis = (y_pred != y)         if mis.any():             ax.scatter(X[mis,0], X[mis,1], s=28, marker='x', label='Misclass.')          # reta de decis\u00e3o         w = ws[frame]; b = bs[frame]         xs, ys, x_vertical = compute_decision_line(w, b, x_min, x_max)         if xs is not None:             ax.plot(xs, ys, lw=2, label='Limite de decis\u00e3o')         elif x_vertical is not None:             ax.axvline(x_vertical, lw=2, label='Limite de decis\u00e3o')          # t\u00edtulo com \u00e9poca e acur\u00e1cia         acc = history[\"accs\"][frame]         ax.set_title(f\"{title} \u2014 \u00c9poca {frame+1}/{epochs} \u00b7 acc={acc:.3f}\")         ax.legend(loc=\"best\")         return []      anim = animation.FuncAnimation(         fig, update, init_func=init, frames=epochs, interval=1000//fps, blit=False     )      # salva GIF     anim.save(out_gif, writer='pillow', fps=fps)     plt.close(fig) <p>=============== um atalho para rodar experimento completo ===============</p> In\u00a0[\u00a0]: Copied! <pre>def run_experiment_with_gif(tag, mean0, cov0, mean1, cov1, n=1000, lr=0.01, max_epochs=100,\n                            seed_data=42, seed_model=0, gif_name=\"out.gif\", fps=8):\n    print(f\"\\n===== {tag} =====\")\n    X, y = make_dataset(mean0, cov0, mean1, cov1, n_per_class=n, seed=seed_data)\n\n    # distribui\u00e7\u00e3o (est\u00e1tico)\n    plt.figure(figsize=(6,6))\n    plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0')\n    plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1')\n    plt.title(f\"{tag}: Distribui\u00e7\u00e3o de Dados\")\n    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n    plt.tight_layout(); plt.show()\n\n    # treino\n    hist = perceptron_fit(X, y, lr=lr, max_epochs=max_epochs, seed=seed_model)\n\n    # gif da reta por \u00e9poca\n    make_gif(X, y, hist, gif_name, f\"{tag}: Reta por \u00c9poca\", fps=fps)\n\n    # curva de acur\u00e1cia (est\u00e1tico)\n    plot_accuracy_curve(hist[\"accs\"], f\"{tag}: Acur\u00e1cia por \u00c9poca\")\n\n    # m\u00e9tricas no console\n    mis = (hist[\"y_pred_final\"] != y)\n    print(\"Convergiu:\", hist[\"converged\"], \"| \u00c9pocas:\", hist[\"epochs_run\"])\n    print(\"w_final:\", np.round(hist[\"w_final\"], 6), \"| b_final:\", round(hist[\"b_final\"], 6))\n    print(\"Acur\u00e1cia final:\", round(hist[\"accs\"][-1], 6))\n    print(\"Erros:\", int(mis.sum()), \"de\", len(y))\n    print(\"GIF salvo em:\", gif_name)\n</pre> def run_experiment_with_gif(tag, mean0, cov0, mean1, cov1, n=1000, lr=0.01, max_epochs=100,                             seed_data=42, seed_model=0, gif_name=\"out.gif\", fps=8):     print(f\"\\n===== {tag} =====\")     X, y = make_dataset(mean0, cov0, mean1, cov1, n_per_class=n, seed=seed_data)      # distribui\u00e7\u00e3o (est\u00e1tico)     plt.figure(figsize=(6,6))     plt.scatter(X[y==0,0], X[y==0,1], s=8, label='Classe 0')     plt.scatter(X[y==1,0], X[y==1,1], s=8, label='Classe 1')     plt.title(f\"{tag}: Distribui\u00e7\u00e3o de Dados\")     plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()     plt.tight_layout(); plt.show()      # treino     hist = perceptron_fit(X, y, lr=lr, max_epochs=max_epochs, seed=seed_model)      # gif da reta por \u00e9poca     make_gif(X, y, hist, gif_name, f\"{tag}: Reta por \u00c9poca\", fps=fps)      # curva de acur\u00e1cia (est\u00e1tico)     plot_accuracy_curve(hist[\"accs\"], f\"{tag}: Acur\u00e1cia por \u00c9poca\")      # m\u00e9tricas no console     mis = (hist[\"y_pred_final\"] != y)     print(\"Convergiu:\", hist[\"converged\"], \"| \u00c9pocas:\", hist[\"epochs_run\"])     print(\"w_final:\", np.round(hist[\"w_final\"], 6), \"| b_final:\", round(hist[\"b_final\"], 6))     print(\"Acur\u00e1cia final:\", round(hist[\"accs\"][-1], 6))     print(\"Erros:\", int(mis.sum()), \"de\", len(y))     print(\"GIF salvo em:\", gif_name) <p>=============== par\u00e2metros do enunciado + execu\u00e7\u00e3o ===============</p> In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Exerc\u00edcio 1 \u2014 separ\u00e1vel\n    EX1_mean0 = [1.5, 1.5]\n    EX1_cov0  = [[0.5, 0], [0, 0.5]]\n    EX1_mean1 = [5.0, 5.0]\n    EX1_cov1  = [[0.5, 0], [0, 0.5]]\n\n    # Exerc\u00edcio 2 \u2014 sobreposi\u00e7\u00e3o\n    EX2_mean0 = [3.0, 3.0]\n    EX2_cov0  = [[1.5, 0], [0, 1.5]]\n    EX2_mean1 = [4.0, 4.0]\n    EX2_cov1  = [[1.5, 0], [0, 1.5]]\n\n    LR = 0.01\n    EPOCHS = 100\n    N = 1000\n\n    run_experiment_with_gif(\"Exerc\u00edcio 1\", EX1_mean0, EX1_cov0, EX1_mean1, EX1_cov1,\n                            n=N, lr=LR, max_epochs=EPOCHS, seed_data=123, seed_model=0,\n                            gif_name=\"ex1.gif\", fps=8)\n\n    run_experiment_with_gif(\"Exerc\u00edcio 2\", EX2_mean0, EX2_cov0, EX2_mean1, EX2_cov1,\n                            n=N, lr=LR, max_epochs=EPOCHS, seed_data=456, seed_model=0,\n                            gif_name=\"ex2.gif\", fps=8)\n</pre> if __name__ == \"__main__\":     # Exerc\u00edcio 1 \u2014 separ\u00e1vel     EX1_mean0 = [1.5, 1.5]     EX1_cov0  = [[0.5, 0], [0, 0.5]]     EX1_mean1 = [5.0, 5.0]     EX1_cov1  = [[0.5, 0], [0, 0.5]]      # Exerc\u00edcio 2 \u2014 sobreposi\u00e7\u00e3o     EX2_mean0 = [3.0, 3.0]     EX2_cov0  = [[1.5, 0], [0, 1.5]]     EX2_mean1 = [4.0, 4.0]     EX2_cov1  = [[1.5, 0], [0, 1.5]]      LR = 0.01     EPOCHS = 100     N = 1000      run_experiment_with_gif(\"Exerc\u00edcio 1\", EX1_mean0, EX1_cov0, EX1_mean1, EX1_cov1,                             n=N, lr=LR, max_epochs=EPOCHS, seed_data=123, seed_model=0,                             gif_name=\"ex1.gif\", fps=8)      run_experiment_with_gif(\"Exerc\u00edcio 2\", EX2_mean0, EX2_cov0, EX2_mean1, EX2_cov1,                             n=N, lr=LR, max_epochs=EPOCHS, seed_data=456, seed_model=0,                             gif_name=\"ex2.gif\", fps=8)"},{"location":"roteiro2/main/","title":"Relat\u00f3rio \u2014 Perceptron e suas Limita\u00e7\u00f5es","text":""},{"location":"roteiro2/main/#parte-1-dados-linearmente-separaveis","title":"Parte 1 \u2014 Dados Linearmente Separ\u00e1veis","text":""},{"location":"roteiro2/main/#geracao-de-dados","title":"Gera\u00e7\u00e3o de Dados","text":"<p>Foram geradas 2.000 amostras (1.000 por classe) a partir de distribui\u00e7\u00f5es normais multivariadas: - Classe 0: m\u00e9dia <code>[1.5, 1.5]</code>, covari\u00e2ncia <code>[[0.5, 0], [0, 0.5]]</code>. - Classe 1: m\u00e9dia <code>[5, 5]</code>, covari\u00e2ncia <code>[[0.5, 0], [0, 0.5]]</code>.</p> <p>Esses par\u00e2metros garantem separabilidade linear quase perfeita, com sobreposi\u00e7\u00e3o m\u00ednima.</p> <p></p>"},{"location":"roteiro2/main/#treinamento-do-perceptron","title":"Treinamento do Perceptron","text":"<ul> <li>Pesos inicializados pr\u00f3ximos de zero.</li> <li>Taxa de aprendizado: \u03b7 = 0.01.</li> <li>Crit\u00e9rio de parada: converg\u00eancia (nenhuma atualiza\u00e7\u00e3o numa \u00e9poca) ou 100 \u00e9pocas.</li> </ul>"},{"location":"roteiro2/main/#resultados","title":"Resultados","text":"<ul> <li>Converg\u00eancia em 4 \u00e9pocas.  </li> <li>Pesos finais: <code>w = [0.0369, 0.0675]</code>.  </li> <li>Bias final: <code>b = -0.33</code>.  </li> <li>Acur\u00e1cia final: 100% (2000/2000 corretos).  <pre><code>===== Exerc\u00edcio 1 =====\nConvergiu: True | \u00c9pocas: 4\nw_final: [0.036892 0.067545] | b_final: -0.33\nAcur\u00e1cia final: 1.0\nErros: 0 de 2000\nGIF salvo em: ex1.gif\n</code></pre> </li> </ul> <p>Curva de acur\u00e1cia ao longo das \u00e9pocas:</p> <p></p> <p>GIF mostrando a reta de decis\u00e3o mudando a cada \u00e9poca:</p> <p></p>"},{"location":"roteiro2/main/#discussao","title":"Discuss\u00e3o","text":"<ul> <li>O Perceptron convergiu rapidamente porque os dados s\u00e3o linearmente separ\u00e1veis.</li> <li>A fronteira de decis\u00e3o encontrada separa perfeitamente as duas classes.</li> <li>Mostra a efic\u00e1cia do Perceptron em cen\u00e1rios simples.</li> </ul>"},{"location":"roteiro2/main/#parte-2-dados-parcialmente-sobrepostos","title":"Parte 2 \u2014 Dados Parcialmente Sobrepostos","text":""},{"location":"roteiro2/main/#geracao-de-dados_1","title":"Gera\u00e7\u00e3o de Dados","text":"<p>Foram geradas 2.000 amostras (1.000 por classe) a partir de distribui\u00e7\u00f5es normais multivariadas: - Classe 0: m\u00e9dia <code>[3, 3]</code>, covari\u00e2ncia <code>[[1.5, 0], [0, 1.5]]</code>. - Classe 1: m\u00e9dia <code>[4, 4]</code>, covari\u00e2ncia <code>[[1.5, 0], [0, 1.5]]</code>.</p> <p>Neste caso, as m\u00e9dias est\u00e3o mais pr\u00f3ximas e a vari\u00e2ncia \u00e9 maior, causando sobreposi\u00e7\u00e3o significativa.</p> <p></p>"},{"location":"roteiro2/main/#treinamento-do-perceptron_1","title":"Treinamento do Perceptron","text":"<ul> <li>Mesmo setup do exerc\u00edcio anterior.</li> <li>Executado por at\u00e9 100 \u00e9pocas.</li> </ul>"},{"location":"roteiro2/main/#resultados_1","title":"Resultados","text":"<ul> <li>N\u00e3o convergiu em 100 \u00e9pocas.  </li> <li>Pesos finais: <code>w = [0.0637, 0.0696]</code>.  </li> <li>Bias final: <code>b = -0.42</code>.  </li> <li>Acur\u00e1cia final: ~69.6% (1392/2000 corretos, 608 erros).  <pre><code>===== Exerc\u00edcio 2 =====\nConvergiu: False | \u00c9pocas: 100\nw_final: [0.063667 0.069554] | b_final: -0.42\nAcur\u00e1cia final: 0.696\nErros: 608 de 2000\nGIF salvo em: ex2.gif\n</code></pre> </li> </ul> <p>Curva de acur\u00e1cia mostra oscila\u00e7\u00f5es constantes:</p> <p></p> <p>GIF mostrando a reta de decis\u00e3o mudando mas sem encontrar separa\u00e7\u00e3o perfeita:</p> <p></p>"},{"location":"roteiro2/main/#discussao_1","title":"Discuss\u00e3o","text":"<ul> <li>O Perceptron n\u00e3o atingiu converg\u00eancia, pois os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis.</li> <li>A acur\u00e1cia m\u00e1xima ficou limitada (~70%), com grande n\u00famero de pontos mal classificados.</li> <li>Esse resultado ilustra bem a limita\u00e7\u00e3o do Perceptron quando a hip\u00f3tese de separabilidade linear n\u00e3o \u00e9 atendida.</li> </ul>"},{"location":"roteiro2/main/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<ul> <li>Parte 1 (Linearmente Separ\u00e1veis): o Perceptron convergiu r\u00e1pido e obteve acur\u00e1cia de 100%.  </li> <li>Parte 2 (N\u00e3o Linearmente Separ\u00e1veis): o Perceptron n\u00e3o convergiu e ficou limitado a ~70% de acur\u00e1cia.  </li> </ul> <p>Esses experimentos mostram que o Perceptron \u00e9 eficiente apenas em cen\u00e1rios onde existe uma fronteira linear clara. Para problemas mais complexos, s\u00e3o necess\u00e1rias arquiteturas mais sofisticadas, como MLPs (Multi-Layer Perceptrons) ou outros modelos de classifica\u00e7\u00e3o.</p>"},{"location":"roteiro2/main/#codigo-fonte","title":"C\u00f3digo-Fonte","text":"<p>A seguir apresentamos a implementa\u00e7\u00e3o do Perceptron seguindo os 7 passos do processo de treinamento (inicializa\u00e7\u00e3o, fornecimento dos dados, forward pass, c\u00e1lculo do erro, atualiza\u00e7\u00e3o dos par\u00e2metros, itera\u00e7\u00e3o por \u00e9pocas e verifica\u00e7\u00e3o de converg\u00eancia). O c\u00f3digo foi separado em fun\u00e7\u00f5es para facilitar a visualiza\u00e7\u00e3o de cada etapa.</p> <pre><code>    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import animation\n\n    def set_seed(seed=42):\n        np.random.seed(seed)\n\n    def make_dataset(mean0, cov0, mean1, cov1, n_per_class=1000, seed=42):\n        set_seed(seed)\n        X0 = np.random.multivariate_normal(mean0, cov0, n_per_class)\n        X1 = np.random.multivariate_normal(mean1, cov1, n_per_class)\n        X = np.vstack([X0, X1])\n        y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)]).astype(int)\n        # embaralha\n        idx = np.arange(len(X))\n        np.random.shuffle(idx)\n        return X[idx], y[idx]\n\n    def perceptron_fit(X, y, lr=0.01, max_epochs=100, seed=0):\n        \"\"\"\n        Treina perceptron online tradicional (sem classe).\n        Retorna um dicion\u00e1rio com hist\u00f3rico para animar/avaliar.\n        Perceptron online: atualiza w,b a cada amostra.\n        \"\"\"\n        set_seed(seed) # para reprodutibilidade\n        n, d = X.shape # n amostras, d dimens\u00f5es (features)\n        w = np.random.randn(d) * 0.01 # inicializa w pequeno\n        b = 0.0                     # inicializa b = 0 (bias)\n\n        ws, bs, accs, preds_hist = [], [], [], [] # hist\u00f3rico por \u00e9poca\n        converged = False # se parou antes do max_epochs\n        epochs_run = max_epochs # quantas \u00e9pocas rodou de fato \n\n        for epoch in range(1, max_epochs + 1): \n            updates = 0\n            for i in range(n):\n                xi = X[i] # amostra i\n                yi = y[i] # label i\n                y_hat = 1 if (np.dot(xi, w) + b) &gt;= 0 else 0 # predi\u00e7\u00e3o (y predito bin\u00e1rio)\n                if y_hat != yi: # s\u00f3 atualiza se errou\n                    delta = (yi - y_hat)          # +1 ou -1\n                    w = w + lr * delta * xi # atualiza w\n                    b = b + lr * delta    # atualiza b\n                    updates += 1\n\n            # fim da \u00e9poca: calcula acur\u00e1cia\n\n            scores = X @ w + b \n            '''\n            @ em Python faz multiplica\u00e7\u00e3o de matrizes (produto matricial)\n            X: geralmente \u00e9 uma matriz (por exemplo, de dados de entrada, onde cada linha \u00e9 \n                                        uma amostra e cada coluna \u00e9 uma caracter\u00edstica/feature).\n            w: normalmente \u00e9 um vetor de pesos (ou coeficientes) para cada caracter\u00edstica.\n            b: \u00e9 um vi\u00e9s (bias), geralmente um escalar ou vetor.\n\n            Calcula uma combina\u00e7\u00e3o linear dos dados de entrada (X), ponderada pelos pesos (w),\n            e soma o vi\u00e9s (b). O resultado (scores) normalmente representa as predi\u00e7\u00f5es de um \n            modelo linear, como regress\u00e3o linear ou a camada de sa\u00edda de uma rede neural simples.\n            '''\n            y_pred = (scores &gt;= 0).astype(int)  # essa linha transforma os scores lineares em predi\u00e7\u00f5es bin\u00e1rias (0 ou 1), \n                                                # usando a fun\u00e7\u00e3o degrau (step function) do perceptron.\n            '''\n            o score pode ser negativo, zero ou positivo dependendo de onde o ponto\n            x est\u00e1 em rela\u00e7\u00e3o \u00e0 reta (no 2D) ou hiperplano (em dimens\u00f5es maiores)\n            O zero \u00e9 exatamente a fronteira de decis\u00e3o: os pontos que satisfazem\n            w\u22c5x+b=0\n            est\u00e3o em cima da reta de decis\u00e3o.\n            Se um ponto d\u00e1 score &gt; 0, ele est\u00e1 de um lado da reta.\n            Se d\u00e1 score &lt; 0, ele est\u00e1 do outro lado.\n            '''\n            acc = (y_pred == y).mean() # acur\u00e1cia da \u00e9poca, m\u00e9dia de acertos\n                                    # (y_pred==y) \u00e9 um array booleano, compara elemento a elemento,\n                                    # True=acertou, False=errou; .mean() d\u00e1 a m\u00e9dia de acertos (True=1, False=0)\n            # salva hist\u00f3rico\n            ws.append(w.copy())\n            bs.append(float(b))\n            accs.append(float(acc))\n            preds_hist.append(y_pred)\n            # crit\u00e9rio de parada: se n\u00e3o teve atualiza\u00e7\u00e3o, parou\n            # como assim se nao teve atualizacao? se o perceptron acertou todas as amostras\n            if updates == 0:\n                converged = True # parou antes do max_epochs\n                epochs_run = epoch # quantas \u00e9pocas rodou de fato\n                break\n\n        return {\n            \"ws\": ws,             # lista de w por \u00e9poca (np.array shape (2,))\n            \"bs\": bs,             # lista de b por \u00e9poca (float)\n            \"accs\": accs,         # acur\u00e1cia por \u00e9poca\n            \"preds_hist\": preds_hist,  # predi\u00e7\u00f5es por \u00e9poca\n            \"converged\": converged,\n            \"epochs_run\": epochs_run,\n            \"w_final\": ws[-1],\n            \"b_final\": bs[-1],\n            \"y_pred_final\": preds_hist[-1],\n        }\n</code></pre>"},{"location":"roteiro3/main/","title":"Relat\u00f3rio - Multi-Layer Perceptrons (MLPs)","text":"<p>Este relat\u00f3rio apresenta os resultados obtidos nos exerc\u00edcios 2, 3 e 4, que envolvem a implementa\u00e7\u00e3o e avalia\u00e7\u00e3o de redes neurais do tipo Multi-Layer Perceptron (MLP) aplicadas a problemas de classifica\u00e7\u00e3o bin\u00e1ria e multiclasse.</p>"},{"location":"roteiro3/main/#exercicio-1-forward-e-backpropagation-manual","title":"Exerc\u00edcio 1 - Forward e Backpropagation Manual","text":"<p>Configura\u00e7\u00e3o inicial: - Entrada: \\(x = [0.5, -0.2]\\) - Sa\u00edda esperada: \\(y = 1.0\\) - Pesos e vieses:   [   W^{(1)} =   \\begin{bmatrix}   0.3 &amp; -0.1 \\   0.2 &amp; 0.4   \\end{bmatrix}, \\quad   b^{(1)} = [0.1, -0.2]   ]   [   W^{(2)} =   \\begin{bmatrix}   0.5 \\   -0.3   \\end{bmatrix}, \\quad   b^{(2)} = 0.2   ] - Taxa de aprendizado: \\(\\eta = 0.3\\) - Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: \\(\\tanh\\) - Fun\u00e7\u00e3o de custo: MSE</p>"},{"location":"roteiro3/main/#forward-pass","title":"Forward Pass","text":"<ul> <li>Pr\u00e9-ativa\u00e7\u00e3o camada oculta:   [   z^{(1)} = xW^{(1)} + b^{(1)} = [0.21, -0.33]   ]</li> <li>Ativa\u00e7\u00e3o camada oculta:   [   h = \\tanh(z^{(1)}) = [0.206966, -0.318521]   ]</li> <li>Pr\u00e9-ativa\u00e7\u00e3o sa\u00edda:   [   z^{(2)} = hW^{(2)} + b^{(2)} = 0.399039   ]</li> <li>Sa\u00edda final:   [   \\hat{y} = \\tanh(z^{(2)}) = 0.379127   ]</li> <li>Fun\u00e7\u00e3o de perda:   [   L = \\tfrac{1}{2}(y - \\hat{y})^2 = 0.192742   ]</li> </ul>"},{"location":"roteiro3/main/#backpropagation","title":"Backpropagation","text":"<ul> <li>Gradiente na sa\u00edda:   [   \\delta^{(2)} = ( \\hat{y} - y ) \\cdot (1 - \\tanh<sup>2(z</sup>)) = -0.531631   ]</li> <li>Gradientes da camada de sa\u00edda:   [   \\frac{\\partial L}{\\partial W^{(2)}} =   \\begin{bmatrix}   -0.110030 \\    0.169335   \\end{bmatrix}, \\quad   \\frac{\\partial L}{\\partial b^{(2)}} = -0.531631   ]</li> <li>Gradiente propagado para camada oculta:   [   \\delta^{(1)} = [-0.254429, \\; 0.143308]   ]</li> <li>Gradientes da camada oculta:   [   \\frac{\\partial L}{\\partial W^{(1)}} =   \\begin{bmatrix}   -0.127215 &amp; 0.071654 \\    0.050886 &amp; -0.028662   \\end{bmatrix}, \\quad   \\frac{\\partial L}{\\partial b^{(1)}} = [-0.254429, 0.143308]   ]</li> </ul>"},{"location":"roteiro3/main/#atualizacao-dos-parametros-eta-03","title":"Atualiza\u00e7\u00e3o dos Par\u00e2metros (\\(\\eta = 0.3\\))","text":"<ul> <li>Novos pesos e vieses ap\u00f3s atualiza\u00e7\u00e3o:</li> </ul> \\[ W^{(2)}_{\\text{new}} = \\begin{bmatrix} 0.533009 \\\\ -0.350801 \\end{bmatrix}, \\quad b^{(2)}_{\\text{new}} = 0.359489 \\] \\[ W^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.338164 &amp; -0.121496 \\\\ 0.184734 &amp; 0.408598 \\end{bmatrix}, \\quad b^{(1)}_{\\text{new}} = [0.176329, -0.242992] \\]"},{"location":"roteiro3/main/#forward-apos-atualizacao","title":"Forward Ap\u00f3s Atualiza\u00e7\u00e3o","text":"<ul> <li>Nova sa\u00edda:   [   \\hat{y}' = 0.570172   ]</li> <li>Novo erro:   [   L' = 0.092376   ]</li> </ul>"},{"location":"roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Ap\u00f3s uma \u00fanica atualiza\u00e7\u00e3o de pesos e vieses, a perda caiu de 0.1927 para 0.0924, mostrando que o processo de backpropagation e gradient descent ajustou corretamente os par\u00e2metros, aproximando a sa\u00edda \\(\\hat{y}\\) do valor esperado \\(y\\).</p>"},{"location":"roteiro3/main/#exercicio-2-classificacao-binaria","title":"Exerc\u00edcio 2 - Classifica\u00e7\u00e3o Bin\u00e1ria","text":"<p>Arquitetura: 2 neur\u00f4nios de entrada, 1 camada oculta e 1 neur\u00f4nio de sa\u00edda (ativa\u00e7\u00e3o <code>tanh</code>).</p> <ul> <li>Curva de Loss: </li> </ul> <p>Observa-se uma redu\u00e7\u00e3o constante da fun\u00e7\u00e3o de perda, estabilizando pr\u00f3ximo de 0.54 ap\u00f3s 300 \u00e9pocas.</p> <ul> <li> <p>Matriz de Confus\u00e3o: </p> </li> <li> <p>Classe 0: 90 acertos, 9 erros.  </p> </li> <li> <p>Classe 1: 136 acertos, 57 erros.</p> </li> <li> <p>M\u00e9tricas principais:</p> </li> </ul> Classe Precis\u00e3o Recall F1-score 0 0.6122 0.9091 0.7317 1 0.9379 0.7047 0.8047 <p>Acur\u00e1cia total: 77.4%</p>"},{"location":"roteiro3/main/#exercicio-3-classificacao-em-3-classes","title":"Exerc\u00edcio 3 - Classifica\u00e7\u00e3o em 3 Classes","text":"<p>Arquitetura: 2 neur\u00f4nios de entrada, 1 camada oculta com mais unidades, sa\u00edda com 3 neur\u00f4nios (ativa\u00e7\u00e3o <code>tanh</code>).</p> <ul> <li>Curva de Loss: </li> </ul> <p>A perda cai at\u00e9 aproximadamente 0.56, mostrando boa converg\u00eancia.</p> <ul> <li> <p>Matriz de Confus\u00e3o: </p> </li> <li> <p>Classe 0: 85 acertos, 15 erros.  </p> </li> <li>Classe 1: 70 acertos, 30 erros.  </li> <li> <p>Classe 2: 74 acertos, 26 erros.  </p> </li> <li> <p>M\u00e9tricas principais:</p> </li> </ul> Classe Precis\u00e3o Recall F1-score 0 0.7944 0.8500 0.8213 1 0.8140 0.7000 0.7527 2 0.6916 0.7400 0.7150 <p>Acur\u00e1cia total: 76.3%</p>"},{"location":"roteiro3/main/#exercicio-4-mlp-mais-profundo-3-classes","title":"Exerc\u00edcio 4 - MLP Mais Profundo (3 Classes)","text":"<p>Arquitetura: semelhante ao Ex3, mas com duas camadas ocultas, aumentando a profundidade da rede.</p> <ul> <li>Curva de Loss: </li> </ul> <p>O modelo converge mais r\u00e1pido e atinge valores menores de perda (~0.44).</p> <ul> <li> <p>Matriz de Confus\u00e3o: </p> </li> <li> <p>Classe 0: 91 acertos, 9 erros.  </p> </li> <li>Classe 1: 75 acertos, 25 erros.  </li> <li> <p>Classe 2: 78 acertos, 22 erros.  </p> </li> <li> <p>M\u00e9tricas principais:</p> </li> </ul> Classe Precis\u00e3o Recall F1-score 0 0.8505 0.9100 0.8792 1 0.8065 0.7500 0.7772 2 0.7800 0.7800 0.7800 <p>Acur\u00e1cia total: 81.3%</p>"},{"location":"roteiro3/main/#comparativo-geral","title":"Comparativo Geral","text":"<ul> <li> <p>Curvas de Loss (Ex2, Ex3, Ex4): </p> </li> <li> <p>O modelo mais simples (Ex2) j\u00e1 obteve desempenho razo\u00e1vel (77%), mas com desequil\u00edbrio entre classes.  </p> </li> <li>O Ex3 ampliou a rede para 3 classes, mantendo acur\u00e1cia pr\u00f3xima de 76%.  </li> <li>O Ex4, com arquitetura mais profunda, apresentou o melhor resultado (81%), al\u00e9m de converg\u00eancia mais r\u00e1pida.  </li> </ul> <p>Conclus\u00e3o: o aumento de profundidade melhorou a capacidade de generaliza\u00e7\u00e3o da rede, reduzindo a perda final e aumentando a acur\u00e1cia geral.</p>"},{"location":"roteiro4/limit.def/","title":"Limit.def","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom io import StringIO\n</pre> import matplotlib.pyplot as plt import numpy as np from io import StringIO In\u00a0[\u00a0]: Copied! <pre>eq = lambda x: np.exp(x)\n</pre> eq = lambda x: np.exp(x) In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(-.2, 2.1)\n</pre> x = np.linspace(-.2, 2.1) In\u00a0[\u00a0]: Copied! <pre>plt.rcParams[\"figure.figsize\"] = (15, 5)\n</pre> plt.rcParams[\"figure.figsize\"] = (15, 5) In\u00a0[\u00a0]: Copied! <pre>xa = 1.5\nya = 7\nk = 0.3\nka = xa - k\nak = xa + k\n</pre> xa = 1.5 ya = 7 k = 0.3 ka = xa - k ak = xa + k In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1, 3)\nfor i in range(3):\n  ax[i].axhline(0, color='gray') # x = 0\n  ax[i].axvline(0, color='gray') # y = 0\n  ax[i].spines['top'].set_visible(False)\n  ax[i].spines['right'].set_visible(False)\n  ax[i].spines['bottom'].set_visible(False)\n  ax[i].spines['left'].set_visible(False)\n  ax[i].plot(x, eq(x), '-r', lw=4)\n  ax[i].set_xlim(min(x), max(x))\n  ax[i].set_xticks([])\n  ax[i].set_yticks([])\n  ax[i].plot([ka, ka], [0, eq(ka)], 'g:')\n  ax[i].plot([0, ka], [eq(ka), eq(ka)], 'g:')\n  ax[i].plot([ak, ak], [0, eq(ak)], 'g:')\n  ax[i].plot([0, ak], [eq(ak), eq(ak)], 'g:')\n  ax[i].text(xa, -0.5, 'a', horizontalalignment='center', fontsize=15)\n  ax[i].text(ka, -0.5, '$a-\\delta$', horizontalalignment='center', fontsize=15)\n  ax[i].text(ak, -0.5, '$a+\\delta$', horizontalalignment='center', fontsize=15)\n  ax[i].text(0, eq(ka), '$L-\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)\n  ax[i].text(0, eq(ak), '$L+\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)\n</pre> fig, ax = plt.subplots(1, 3) for i in range(3):   ax[i].axhline(0, color='gray') # x = 0   ax[i].axvline(0, color='gray') # y = 0   ax[i].spines['top'].set_visible(False)   ax[i].spines['right'].set_visible(False)   ax[i].spines['bottom'].set_visible(False)   ax[i].spines['left'].set_visible(False)   ax[i].plot(x, eq(x), '-r', lw=4)   ax[i].set_xlim(min(x), max(x))   ax[i].set_xticks([])   ax[i].set_yticks([])   ax[i].plot([ka, ka], [0, eq(ka)], 'g:')   ax[i].plot([0, ka], [eq(ka), eq(ka)], 'g:')   ax[i].plot([ak, ak], [0, eq(ak)], 'g:')   ax[i].plot([0, ak], [eq(ak), eq(ak)], 'g:')   ax[i].text(xa, -0.5, 'a', horizontalalignment='center', fontsize=15)   ax[i].text(ka, -0.5, '$a-\\delta$', horizontalalignment='center', fontsize=15)   ax[i].text(ak, -0.5, '$a+\\delta$', horizontalalignment='center', fontsize=15)   ax[i].text(0, eq(ka), '$L-\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)   ax[i].text(0, eq(ak), '$L+\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15) In\u00a0[\u00a0]: Copied! <pre>ax[0].plot([xa, xa], [0, eq(xa)], 'b:')\nax[0].plot([0, xa], [eq(xa), eq(xa)], 'b:')\nax[0].plot(xa, eq(xa), 'ro', ms=15)\nax[0].text(0, eq(xa), 'L=f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[1].plot([xa, xa], [0, eq(xa)], 'b:')\nax[1].plot([0, xa], [eq(xa), eq(xa)], 'm:')\nax[1].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white')\nax[1].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[2].plot(xa, eq(xa), marker='o', ms=15, mec='white', color='white')\nax[2].plot([xa, xa], [0, ya], 'b:')\nax[2].plot([0, xa], [ya, ya], 'b:')\nax[2].plot([0, xa], [eq(xa), eq(xa)], 'm:')\nax[2].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white')\nax[2].plot(xa, ya, 'ro', ms=15)\nax[2].text(0, ya, 'f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[2].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15)\n</pre> ax[0].plot([xa, xa], [0, eq(xa)], 'b:') ax[0].plot([0, xa], [eq(xa), eq(xa)], 'b:') ax[0].plot(xa, eq(xa), 'ro', ms=15) ax[0].text(0, eq(xa), 'L=f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[1].plot([xa, xa], [0, eq(xa)], 'b:') ax[1].plot([0, xa], [eq(xa), eq(xa)], 'm:') ax[1].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white') ax[1].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[2].plot(xa, eq(xa), marker='o', ms=15, mec='white', color='white') ax[2].plot([xa, xa], [0, ya], 'b:') ax[2].plot([0, xa], [ya, ya], 'b:') ax[2].plot([0, xa], [eq(xa), eq(xa)], 'm:') ax[2].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white') ax[2].plot(xa, ya, 'ro', ms=15) ax[2].text(0, ya, 'f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[2].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15) In\u00a0[\u00a0]: Copied! <pre>fig.tight_layout()\n</pre> fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>buffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"roteiro4/main/","title":"Relat\u00f3rio","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-09-29T19:39:02.051222 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ 2025-09-29T19:39:03.897449 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"roteiro4/smc/","title":"Smc","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom io import StringIO\n</pre> from datetime import datetime import matplotlib.pyplot as plt import yfinance as yf import numpy as np import pandas as pd from io import StringIO In\u00a0[\u00a0]: Copied! <pre>num_days = 250\nnum_simulations = 200\norder_poly = 1\n</pre> num_days = 250 num_simulations = 200 order_poly = 1 In\u00a0[\u00a0]: Copied! <pre>ticker = '^BVSP'\n</pre> ticker = '^BVSP' In\u00a0[\u00a0]: Copied! <pre>info = yf.Ticker(ticker)\ndata = info.history(period='2y')\n</pre> info = yf.Ticker(ticker) data = info.history(period='2y') In\u00a0[\u00a0]: Copied! <pre>close = data['Close']\ndaily_return = close.pct_change()\n</pre> close = data['Close'] daily_return = close.pct_change() In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(1, len(close), len(close))\nf = np.poly1d(np.polyfit(x, close, order_poly))\nxs = np.linspace(max(x), max(x) + num_days, num_days)\n</pre> x = np.linspace(1, len(close), len(close)) f = np.poly1d(np.polyfit(x, close, order_poly)) xs = np.linspace(max(x), max(x) + num_days, num_days) In\u00a0[\u00a0]: Copied! <pre>sigma = daily_return.std()\nmu = daily_return.mean()\n</pre> sigma = daily_return.std() mu = daily_return.mean() In\u00a0[\u00a0]: Copied! <pre>simulated_prices = np.zeros((num_days, num_simulations))\n</pre> simulated_prices = np.zeros((num_days, num_simulations)) In\u00a0[\u00a0]: Copied! <pre>for i in range(num_simulations):\n    simulated_prices[0][i] = close[-1]\n    for j in range(1, num_days):\n        daily_return = np.random.normal(mu, sigma)\n        simulated_prices[j][i] = simulated_prices[j-1][i] * (1 + daily_return)\n</pre> for i in range(num_simulations):     simulated_prices[0][i] = close[-1]     for j in range(1, num_days):         daily_return = np.random.normal(mu, sigma)         simulated_prices[j][i] = simulated_prices[j-1][i] * (1 + daily_return) In\u00a0[\u00a0]: Copied! <pre>simulated_means = np.mean(simulated_prices, axis=1)\nsimulated_stds = np.std(simulated_prices, axis=1)\n</pre> simulated_means = np.mean(simulated_prices, axis=1) simulated_stds = np.std(simulated_prices, axis=1) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(\n    x, close,\n    x, f(x), 'r:',\n    xs, simulated_prices,\n    xs, simulated_means,\n    xs, simulated_means + 1*simulated_stds, 'w:',\n    xs, simulated_means - 1*simulated_stds, 'w:',\n    xs, simulated_means + 2*simulated_stds, 'k:',\n    xs, simulated_means - 2*simulated_stds, 'k:',\n    xs, f(xs), 'g',\n)\nax.set_xlim(min(x), max(xs))\n</pre> fig, ax = plt.subplots(1, 1, figsize=(12, 8)) ax.plot(     x, close,     x, f(x), 'r:',     xs, simulated_prices,     xs, simulated_means,     xs, simulated_means + 1*simulated_stds, 'w:',     xs, simulated_means - 1*simulated_stds, 'w:',     xs, simulated_means + 2*simulated_stds, 'k:',     xs, simulated_means - 2*simulated_stds, 'k:',     xs, f(xs), 'g', ) ax.set_xlim(min(x), max(xs)) In\u00a0[\u00a0]: Copied! <pre>buffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</pre> buffer = StringIO() plt.savefig(buffer, format=\"svg\") print(buffer.getvalue())"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}