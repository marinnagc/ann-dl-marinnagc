√ìtimo, agora tenho os prints do enunciado com os par√¢metros exatos üëå. Vou te explicar **cada etapa da implementa√ß√£o** que voc√™ precisa fazer, relacionando com os exerc√≠cios 1 e 2.

---

## üîπ 1. Gera√ß√£o dos dados

### Exerc√≠cio 1

* Classe 0:

  * M√©dia = \[1.5, 1.5]
  * Covari√¢ncia = \[\[0.5, 0], \[0, 0.5]] (vari√¢ncia baixa ‚Üí pontos mais concentrados)
* Classe 1:

  * M√©dia = \[5, 5]
  * Covari√¢ncia = \[\[0.5, 0], \[0, 0.5]]

üëâ Resultado: pontos bem separados (linearmente separ√°veis, pouca sobreposi√ß√£o).

### Exerc√≠cio 2

* Classe 0:

  * M√©dia = \[3, 3]
  * Covari√¢ncia = \[\[1.5, 0], \[0, 1.5]] (vari√¢ncia maior ‚Üí pontos espalhados)
* Classe 1:

  * M√©dia = \[4, 4]
  * Covari√¢ncia = \[\[1.5, 0], \[0, 1.5]]

üëâ Resultado: pontos se sobrep√µem mais ‚Üí n√£o d√° para separar perfeitamente com uma linha reta.

### C√≥digo t√≠pico (vale para ambos):

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
n = 1000

# Exemplo do Ex1
mean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]]
mean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]

X0 = np.random.multivariate_normal(mean0, cov0, n)
X1 = np.random.multivariate_normal(mean1, cov1, n)

X = np.vstack([X0, X1])
y = np.hstack([np.zeros(n), np.ones(n)])
```

E plota:

```python
plt.scatter(X0[:,0], X0[:,1], label="Classe 0")
plt.scatter(X1[:,0], X1[:,1], label="Classe 1")
plt.legend(); plt.show()
```

---

## üîπ 2. Implementa√ß√£o do Perceptron

### Estrutura:

* Pesos `w = [w1, w2]` e vi√©s `b`.
* Inicializados aleatoriamente ou zerados.
* Fun√ß√£o de decis√£o:
  $y_{pred} = \text{step}(w \cdot x + b)$

### Regra de atualiza√ß√£o:

Para cada ponto `(x, y)`:

* Calcula predi√ß√£o `≈∑`.
* Se errou (`≈∑ != y`), atualiza:

  $$
  w = w + \eta \cdot (y - \hat{y}) \cdot x
  $$

  $$
  b = b + \eta \cdot (y - \hat{y})
  $$
* Onde `Œ∑` √© a taxa de aprendizado (comece com 0.01).

---

## üîπ 3. Treinamento

* Loop por **√©pocas** (at√© 100).
* Em cada √©poca, percorre todos os exemplos.
* Conta quantas atualiza√ß√µes foram feitas:

  * Se **nenhuma** ‚Üí convergiu.
* Guarda a acur√°cia ap√≥s cada √©poca para plotar curva.

---

## üîπ 4. Avalia√ß√£o

* Depois do treino:

  * Acur√°cia final = n¬∫ de acertos √∑ total.
  * Plote:

    * **Limite de decis√£o**:
      Se w2 ‚â† 0:

      $$
      x2 = -\frac{w1}{w2} \cdot x1 - \frac{b}{w2}
      $$
    * **Curva de acur√°cia** (√©poca √ó acur√°cia).
    * **Pontos errados** com ‚ÄúX‚Äù ou cor diferente.

---

## üîπ 5. Interpreta√ß√£o dos resultados

* **Exerc√≠cio 1**
  Dados separ√°veis ‚Üí perceptron converge r√°pido (100% de acur√°cia).
  Pesos finais definem uma linha clara entre as classes.

* **Exerc√≠cio 2**
  Dados sobrepostos ‚Üí perceptron n√£o chega a 100% de acur√°cia.
  Pode oscilar entre √©pocas.
  Mesmo assim encontra um limite razo√°vel que separa ‚Äúo grosso‚Äù dos dados.

---

## üîπ Fluxo completo resumido

1. Gerar dados com `np.random.multivariate_normal`.
2. Plotar nuvem de pontos.
3. Inicializar perceptron (`w, b`).
4. Loop de treino com regra de atualiza√ß√£o.
5. Calcular acur√°cia por √©poca.
6. Plotar:

   * Dados + linha de decis√£o.
   * Curva de acur√°cia.
   * Pontos errados.
7. Reportar: `w, b, acur√°cia final` + discuss√£o.

---

Quer que eu escreva **um c√≥digo √∫nico**, que roda **Ex1 e Ex2** j√° com esses par√¢metros do enunciado, plota tudo e mostra pesos/acur√°cia final no console?
